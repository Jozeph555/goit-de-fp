# Part I: End-to-End Streaming Pipeline for Athlete Data Processing

## Опис проєкту

Цей проєкт реалізує потокове рішення для обробки даних атлетів у режимі реального часу з використанням Apache Spark Structured Streaming та Apache Kafka. Система призначена для букмекерської контори, яка потребує швидкого оновлення коефіцієнтів ставок на основі статистичних даних спортсменів.

## Архітектура рішення

### Компоненти системи:
- **MySQL база даних** - зберігає біологічні дані атлетів (`athlete_bio`)
- **Apache Kafka** - потокова передача результатів змагань 
- **Apache Spark Streaming** - обробка та агрегація даних в реальному часі
- **Вихідні системи** - Kafka топік та MySQL таблиця для збагачених даних

### Потік даних:
1. Статичні дані атлетів (зріст, вага, стать, країна) зберігаються в MySQL
2. Результати змагань надходять через Kafka топік `athlete_event_results`
3. Spark об'єднує потокові та статичні дані
4. Розраховуються середні показники по групах (вид спорту, медаль, стать, країна)
5. Результати записуються одночасно в Kafka топік та MySQL базу

## Реалізовані етапи

### Етап 1: Читання біологічних даних з MySQL
- Завантаження 155,861 записів з таблиці `athlete_bio`
- Використання JDBC підключення через Spark

### Етап 2: Фільтрація даних
- Видалення записів з порожніми або некоректними значеннями зросту/ваги
- Конвертація строкових значень у числові типи
- Збережено 105,090 записів (67.4% від загальної кількості)

### Етап 3: Читання потокових даних з Kafka
- Підключення до топіку `athlete_event_results`
- Парсинг JSON структури з полями: athlete_id, sport, medal, country_noc, event

### Етап 4: Об'єднання даних
- Inner join потокових даних з біологічними за ключем `athlete_id`
- Збагачення результатів змагань фізичними характеристиками атлетів

### Етап 5: Розрахунок агрегацій
- Групування за: вид спорту, тип медалі, стать, країна (country_noc)
- Обчислення середнього зросту та ваги для кожної групи
- Додавання timestamp обробки

### Етап 6: Запис результатів (FanOut патерн)
**6a) Вихідний Kafka топік `enriched_athlete_data`:**
- JSON формат з розрахованими середніми значеннями

**6b) MySQL таблиця `enriched_athlete_stats`:**
- Структуровані дані для подальшого аналізу

## Результати роботи

### Статистика обробки:
- **Оброблено**: 925,808 вхідних записів результатів змагань
- **Створено**: 27,506 агрегованих записів
- **Найпопулярніші види спорту**: Athletics (2,602), Swimming (1,603), Shooting (1,152)
- **Розподіл медалей**: No Medal (19,256), Bronze (3,107), Silver (2,774), Gold (2,373)

### Приклади збагачених даних:
- **Athletics/Male/BIZ**: середній зріст 175.81 см, вага 66.81 кг
- **Swimming/Male/BAN**: середній зріст 168.86 см, вага 64.86 кг  
- **Taekwondo/Female/ARU**: середній зріст 171.0 см, вага 49.0 кг

## Структура проєкту

```
goit-de-fp/
├── src/
│   ├── main.py              # Основний streaming pipeline
│   └── config.py            # Конфігурація підключень
├── kafka_scripts/
│   ├── producer.py          # Завантаження даних з MySQL в Kafka
│   └── consumer.py          # Тестування вхідного топіку
├── test/
│   ├── consumer_output.py   # Перевірка вихідного топіку
│   └── check_mysql.py       # Перевірка результатів у БД
├── sql/
│   └── create_output_table.sql  # SQL для створення вихідної таблиці
├── jars/
│   └── mysql-connector-j-8.0.32.jar  # JDBC драйвер
└── README.md
```

## Як запустити

### 1. Підготовка середовища
```bash
# Встановлення залежностей
pip install pyspark kafka-python

# Створення директорій
mkdir -p jars checkpoints
```

### 2. Налаштування конфігурації
Відредагуйте `src/config.py` з реальними credentials для MySQL та Kafka.

### 3. Створення вихідної таблиці
```bash
mysql -h host -u username -p database < sql/create_output_table.sql
```

### 4. Завантаження даних в Kafka
```bash
python kafka_scripts/producer.py
```

### 5. Запуск streaming pipeline
```bash
python src/main.py
```

### 6. Перевірка результатів
```bash
# Перевірка вихідного Kafka топіку
python test/consumer_output.py

# Перевірка MySQL бази даних
python test/check_mysql.py
```

## Скріншоти роботи системи

### Скріншот 1: Запуск streaming pipeline
Показує успішне виконання всіх 6 етапів:
- Читання 155,861 біологічних записів з MySQL
- Фільтрацію до 105,090 валідних записів (67.4%)
- Налаштування потокової обробки Kafka

### Скріншоти 2-4: Обробка батчів в реальному часі
Демонструють роботу forEachBatch функції:
- Batch 1: 82 записи → відправлено в Kafka та збережено в MySQL
- Batch 2: 93 записи з прикладами агрегованих даних
- Постійна обробка з виводом прогресу "Sent 490000 records..."

### Скріншоти 5-7: Перевірка вихідного Kafka топіку
Результат роботи `consumer_output.py`:
- 10 збагачених повідомлень з топіку `enriched_athlete_data`
- Структура: Sport, Medal, Sex, Country, Avg Height, Avg Weight, Timestamp
- Приклади: Athletics/ANZ (170.83см, 66кг), Swimming/BAN (168.86см, 64.86кг)

### Скріншоти 8-10: Перевірка MySQL бази даних
Результат роботи `check_mysql.py`:
- Всього записів в `enriched_athlete_stats`: 27,506
- Топ види спорту: Athletics (2,602), Swimming (1,603), Shooting (1,152)
- Останні оброблені записи з timestamp 2025-09-27 17:48:11

## Технічні особливості

### Використані технології:
- **PySpark 4.0.0** - для потокової обробки
- **Apache Kafka** - для потокової передачі даних
- **MySQL** - для зберігання статичних та результуючих даних
- **JDBC** - для підключення Spark до MySQL

### Ключові функції:
- **forEachBatch** - для одночасного запису в кілька джерел
- **Structured Streaming** - для обробки потокових даних
- **JSON parsing** - для роботи з Kafka повідомленнями
- **Aggregation** - для розрахунку статистик по групах

### Налаштування продуктивності:
- Обробка по 1000 записів за раз (`maxOffsetsPerTrigger`)
- Тригер кожні 10 секунд (`processingTime`)
- Checkpoint для відновлення після збоїв

## Висновки

Система успішно демонструє повний цикл потокової обробки даних від джерела до кінцевих споживачів з можливістю масштабування для обробки великих обсягів даних в режимі реального часу. Реалізований FanOut патерн забезпечує одночасний запис результатів у різні системи, що підвищує надійність та гнучкість архітектури.
