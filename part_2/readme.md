# Batch Data Lake ETL Pipeline

End-to-End пакетний конвеєр обробки даних про олімпійських атлетів з використанням Apache Spark та Apache Airflow.

## Огляд проєкту

Проєкт реалізує багаторівневу архітектуру озера даних (Bronze → Silver → Gold) для обробки біографічних даних олімпійських атлетів та результатів змагань.

## Архітектура

```
Landing → Bronze → Silver → Gold
  (CSV)   (Parquet) (Очищені) (Аналітика)
```

### Рівні даних

- **Bronze Layer**: Сирі дані з FTP у форматі Parquet
- **Silver Layer**: Очищені та дедупліковані дані
- **Gold Layer**: Агреговані аналітичні дані з середніми статистиками

## Компоненти конвеєра

### 1. Landing to Bronze (`landing_to_bronze.py`)
- Завантажує CSV файли з FTP сервера
- Конвертує в формат Parquet
- Зберігає в директорію `bronze/`

**Джерела даних:**
- `athlete_bio.csv` - біографічні дані атлетів
- `athlete_event_results.csv` - результати змагань

### 2. Bronze to Silver (`bronze_to_silver.py`)
- Очищає текстові колонки (видаляє спеціальні символи)
- Видаляє дублікати записів
- Зберігає в директорію `silver/`

### 3. Silver to Gold (`silver_to_gold.py`)
- Об'єднує біографічні дані та результати
- Конвертує вагу/зріст у числовий формат
- Обчислює середні статистики за видом спорту, медаллю, статтю та країною
- Додає timestamp для відстеження
- Зберігає в `gold/avg_stats/`

## Оркестрація

**Airflow DAG:** `Yanvari_batch_data_lake_etl`

Задачі виконуються послідовно:
```
landing_to_bronze → bronze_to_silver → silver_to_gold
```

## Технології

- **Apache Spark 3.x** - розподілена обробка даних
- **Apache Airflow 2.x** - оркестрація робочих процесів
- **PySpark** - Python API для Spark
- **Parquet** - колонковий формат зберігання

## Обробка даних

**Вхідні дані:**
- 155,861 записів атлетів
- 315,626 результатів змагань

**Вихідна статистика:**
- 4,342 унікальні комбінації (спорт, медаль, стать, країна)
- Середня вага та зріст за категорією
- Часова мітка для кожного обчислення

## Ключові особливості

- Масштабована розподілена обробка з Spark
- Автоматизована оркестрація робочих процесів з Airflow
- Якість даних: очищення тексту та дедублікація
- Ефективне зберігання з Parquet стисненням
- Можливість інкрементальної обробки

## Конфігурація

Всі константи визначені в скриптах:
- FTP URL: `https://ftp.goit.study/neoversity/`
- Таблиці: `athlete_bio`, `athlete_event_results`
- Директорії: `bronze/`, `silver/`, `gold/`

## Виконання

DAG запускається за розкладом або вручну через інтерфейс Airflow.

Кожна задача логує прогрес та виводить зразки даних для перевірки.
